# üïê Scrapeur Hebdomadaire Automatique

Syst√®me de scraping p√©riodique pour alimenter la base de donn√©es avec des offres fra√Æches.

## üéØ Objectif

Au lieu de scraper √† chaque recherche utilisateur, le scrapeur hebdomadaire :
1. **Tourne automatiquement** chaque semaine (dimanche minuit par d√©faut)
2. **Scrape 20+ requ√™tes pr√©d√©finies** (python, react, devops, data, etc.)
3. **Agr√®ge toutes les sources** : France Travail, Adzuna, EURES, APEC, Indeed, WTTJ, Remotive
4. **D√©duplique et stocke** en BDD
5. **Enrichit** les donn√©es (salaire, remote, skills)

‚úÖ **Avantages** :
- Recherche utilisateur ultra-rapide (lecture BDD)
- Historique des offres
- D√©tection offres retir√©es
- Moins de charge sur les sites externes
- Respect rate limits

## üìã Sources Scrap√©es

### APIs (stubs ou actives)
- ‚úÖ **France Travail API** (stub)
- ‚úÖ **Adzuna API** (stub)
- ‚úÖ **EURES API** (stub)

### Scraping Actif
- ‚úÖ **Welcome to the Jungle** (France, startups)
- ‚úÖ **Remotive.io** (International remote)
- ‚úÖ **APEC** (Cadres France) - **NOUVEAU**
- ‚úÖ **Indeed** (Multi-pays) - **NOUVEAU**

## üöÄ Installation

### 1. Script pr√™t √† l'emploi

Le script est d√©j√† cr√©√© : `backend/run_weekly_scraper.py`

### 2. Test manuel

```bash
cd backend
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

python run_weekly_scraper.py
```

Vous verrez :
```
============================================================
WEEKLY JOB SCRAPER
============================================================
[WeeklyScraper] Starting at 2024-12-19 01:30:00
[WeeklyScraper] Scraping: python developer (fr)
  [APEC] Scraped 12 jobs
  [Indeed] Scraped 15 jobs
  ‚Üí 45 scraped, 38 unique
[WeeklyScraper] Scraping: javascript react (fr)
  ...
[WeeklyScraper] Finished!
  - Total scraped: 876
  - Total stored: 623
  - Errors: 2

============================================================
SUMMARY
============================================================
‚úÖ Scraped: 876 jobs
‚úÖ Stored: 623 unique jobs
‚úÖ No critical errors!
============================================================
```

## ‚è∞ Automatisation Windows (Task Scheduler)

### Cr√©er la t√¢che planifi√©e

1. **Ouvrir Planificateur de t√¢ches** :
   - Win+R ‚Üí `taskschd.msc`

2. **Cr√©er une t√¢che de base** :
   - Nom : `DevJobs Weekly Scraper`
   - D√©clencheur : **Hebdomadaire** ‚Üí Dimanche √† 00:00
   - Action : **D√©marrer un programme**

3. **Programme/script** :
   ```
   C:\Users\User\Desktop\Projets\Kenshu Job\backend\.venv\Scripts\python.exe
   ```

4. **Ajouter des arguments** :
   ```
   run_weekly_scraper.py
   ```

5. **Commencer dans** :
   ```
   C:\Users\User\Desktop\Projets\Kenshu Job\backend
   ```

6. **Options avanc√©es** :
   - ‚òë Ex√©cuter m√™me si l'utilisateur n'est pas connect√©
   - ‚òë Ex√©cuter avec les autorisations maximales
   - ‚òê Ne d√©marrer que si l'ordinateur est branch√© (d√©cocher pour laptop)

7. **Sauvegarder** et tester :
   - Clic droit sur la t√¢che ‚Üí **Ex√©cuter**

### PowerShell one-liner (alternative)

```powershell
$action = New-ScheduledTaskAction -Execute "C:\Users\User\Desktop\Projets\Kenshu Job\backend\.venv\Scripts\python.exe" -Argument "run_weekly_scraper.py" -WorkingDirectory "C:\Users\User\Desktop\Projets\Kenshu Job\backend"
$trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek Sunday -At 00:00
Register-ScheduledTask -TaskName "DevJobs Weekly Scraper" -Action $action -Trigger $trigger -Description "Scraping hebdomadaire offres IT"
```

## üêß Automatisation Linux/Mac (cron)

√âditer crontab :
```bash
crontab -e
```

Ajouter :
```cron
# Chaque dimanche √† 00:00
0 0 * * 0 cd /path/to/backend && .venv/bin/python run_weekly_scraper.py >> /var/log/weekly_scraper.log 2>&1
```

Ou avec `anacron` pour ex√©cution diff√©r√©e si machine √©teinte.

## üìä Configuration Requ√™tes

√âditer `backend/app/scheduler/weekly_scraper.py` :

```python
WEEKLY_QUERIES = [
    {"keywords": "python developer", "countries": ["fr"]},
    {"keywords": "javascript react", "countries": ["fr"]},
    # ... ajouter vos requ√™tes
    {"keywords": "votre technologie", "countries": ["fr", "de"]},
]
```

### Exemples de requ√™tes utiles

```python
# Langages
{"keywords": "python django", "countries": ["fr"]},
{"keywords": "javascript typescript", "countries": ["fr"]},
{"keywords": "java spring boot", "countries": ["fr"]},
{"keywords": "c# dotnet", "countries": ["fr"]},
{"keywords": "php laravel", "countries": ["fr"]},
{"keywords": "ruby rails", "countries": ["fr"]},

# Frameworks frontend
{"keywords": "react nextjs", "countries": ["fr"]},
{"keywords": "vue nuxt", "countries": ["fr"]},
{"keywords": "angular", "countries": ["fr"]},
{"keywords": "svelte", "countries": ["fr"]},

# Mobile
{"keywords": "ios swift swiftui", "countries": ["fr"]},
{"keywords": "android kotlin jetpack", "countries": ["fr"]},
{"keywords": "react native", "countries": ["fr"]},
{"keywords": "flutter", "countries": ["fr"]},

# DevOps/Cloud
{"keywords": "kubernetes docker", "countries": ["fr", "de"]},
{"keywords": "aws architect", "countries": ["fr"]},
{"keywords": "azure devops", "countries": ["fr"]},
{"keywords": "terraform ansible", "countries": ["fr"]},
{"keywords": "gitlab ci jenkins", "countries": ["fr"]},

# Data/AI
{"keywords": "data engineer spark", "countries": ["fr"]},
{"keywords": "data scientist", "countries": ["fr"]},
{"keywords": "machine learning", "countries": ["fr", "us"]},
{"keywords": "mlops", "countries": ["fr"]},
{"keywords": "nlp transformers", "countries": ["fr", "us"]},

# S√©curit√©
{"keywords": "security engineer pentest", "countries": ["fr"]},
{"keywords": "devsecops", "countries": ["fr"]},
{"keywords": "soc analyst", "countries": ["fr"]},

# Autres
{"keywords": "blockchain solidity", "countries": ["fr", "de"]},
{"keywords": "game developer unity", "countries": ["fr"]},
{"keywords": "embedded iot", "countries": ["fr"]},
```

## üîß Monitoring & Logs

### Logs simples

Rediriger stdout vers fichier :

**Windows** (Task Scheduler) :
- Programme : `cmd.exe`
- Arguments : `/c "C:\...\python.exe run_weekly_scraper.py > logs\scraper.log 2>&1"`

**Linux/Mac** :
```bash
0 0 * * 0 cd /path/to/backend && .venv/bin/python run_weekly_scraper.py >> /var/log/weekly_scraper.log 2>&1
```

### Logs rotatifs (recommand√©)

Installer `logrotate` ou √©quivalent Windows pour √©viter logs g√©ants.

### Alertes email

Modifier `run_weekly_scraper.py` pour envoyer email si erreurs :

```python
import smtplib
from email.message import EmailMessage

if result['errors']:
    msg = EmailMessage()
    msg['Subject'] = '‚ö†Ô∏è Weekly Scraper Errors'
    msg['From'] = 'scraper@yourapp.com'
    msg['To'] = 'admin@yourapp.com'
    msg.set_content(f"Errors:\n" + "\n".join(result['errors']))
    
    with smtplib.SMTP('smtp.gmail.com', 587) as smtp:
        smtp.starttls()
        smtp.login('user', 'pass')
        smtp.send_message(msg)
```

## üìà Statistiques

Apr√®s plusieurs semaines, vous aurez :
- **Plusieurs milliers d'offres** en BDD
- **Historique** des postes (date apparition/disparition)
- **Tendances salaires** par techno/r√©gion
- **D√©tection entreprises** qui recrutent activement

## üéØ Next Steps

### 1. Base de Donn√©es Persistante

Remplacer `MemoryStore` par Postgres :

```python
# backend/app/storage/postgres.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

engine = create_engine(os.getenv("DATABASE_URL"))
Session = sessionmaker(bind=engine)

def upsert_jobs(jobs):
    session = Session()
    for job in jobs:
        session.merge(job)  # INSERT or UPDATE
    session.commit()
```

### 2. Historisation

Ajouter champs `first_seen`, `last_seen`, `is_active` :

```python
class JobPosting(BaseModel):
    # ... champs existants
    first_seen: datetime
    last_seen: datetime
    is_active: bool = True
```

### 3. Enrichissement LLM

Apr√®s stockage, lancer enrichissement :

```python
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

for job in new_jobs:
    if not job.salary_min:
        # Extraire salaire depuis description
        prompt = f"Extract salary range from: {job.description}"
        response = client.chat.completions.create(...)
        job.salary_min, job.salary_max = parse_llm_response(response)
```

### 4. Notifications Utilisateurs

Si un utilisateur a sauvegard√© des crit√®res, l'alerter sur nouvelles offres match√©es :

```python
for user in users_with_alerts:
    matching_jobs = [j for j in new_jobs if matches_user_criteria(j, user)]
    if matching_jobs:
        send_email_alert(user, matching_jobs)
```

## ‚ö†Ô∏è Consid√©rations Importantes

### Rate Limiting
- **D√©lais entre requ√™tes** : 1-2 secondes (d√©j√† dans Indeed connector)
- **Rotation IP** : si volume important (proxies)
- **Headers r√©alistes** : User-Agent, Accept, etc.

### Respect CGU
- ‚úÖ APEC : pas de robots.txt bloquant `/recherche-emploi`
- ‚úÖ Indeed : rate limiting respect√©
- ‚úÖ WTTJ : parsing HTML public
- ‚úÖ Remotive : API publique

### Fallback
Le scraper ne doit **jamais bloquer** l'application principale :
- Chaque source est dans un `try/except`
- Erreurs logg√©es mais non critiques
- Pipeline continue m√™me si 1-2 sources √©chouent

---

**üéØ R√©sultat** : Votre BDD se remplit automatiquement chaque semaine avec des centaines d'offres IT fra√Æches, pr√™tes √† √™tre match√©es instantan√©ment avec vos utilisateurs ! üöÄ

